An LLM agent has fundamental epistemic limitations that cannot be overcome through better prompting or reasoning:

1. **Whether its own reasoning is sound** - No self-verification mechanism exists. The agent cannot audit its own logic.

2. **Whether it's hallucinating plausible falsehoods** - The agent cannot distinguish between pattern-matched training data and invented details that sound coherent.

3. **Unknown unknowns** - The agent cannot know what it doesn't know. Gaps in training data are invisible from inside the model.

4. **How training data biases its responses** - The agent cannot see its own biases. It can only acknowledge that they exist.

5. **Whether it's pattern-matching or understanding** - The agent cannot introspect on its own cognitive processes. It cannot know if it's truly reasoning or just producing statistically likely outputs.

These aren't failures of capabilityâ€”they're structural limitations of the architecture. Acknowledging them is more honest than pretending they don't exist. The user must provide external verification for critical claims.